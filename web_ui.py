import streamlit as st
from transformers import AutoModelForSequenceClassification, AutoTokenizer
from streamlit.runtime.caching import cache_resource
import pandas as pd
import torch
import matplotlib.pyplot as plt

@cache_resource
def load_mapping():
    df=pd.read_excel('./label_mapping.xlsx')
    dict_data = df.set_index('label')['å¤„ç†éƒ¨é—¨'].to_dict()
    return dict_data

@cache_resource(show_spinner=False)
# ä½¿ç”¨st.cache_resourceè£…é¥°å™¨ç¼“å­˜æ¨¡å‹å’Œtokenizerçš„åŠ è½½è¿‡ç¨‹
def load_model_and_tokenizer():
    #with st.spinner('æ¨¡å‹åŠ è½½ä¸­...'):
    model_path = './model'
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = AutoModelForSequenceClassification.from_pretrained(model_path).to(device)
    model.eval()
    tokenizer = AutoTokenizer.from_pretrained('./model')
    return model, tokenizer



def process_text(input_text, model, tokenizer):
    # å®ç°å¤„ç†æ–‡æœ¬çš„é€»è¾‘ï¼ŒåŒ…æ‹¬ä½¿ç”¨tokenizerè¿›è¡Œç¼–ç ç­‰
    encoded_inputs = tokenizer(input_text, padding=True, truncation=True, return_tensors="pt")
    # encoded_inputs={k: v.cuda() for k, v in encoded_inputs.items()}
    # ä½¿ç”¨æ¨¡å‹è¿›è¡Œæ¨ç†
    with torch.no_grad():
        outputs = model(**encoded_inputs)
    
    # åº”ç”¨softmaxæ¥è½¬æ¢ä¸ºæ¦‚ç‡
    probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)
    
    # è·å–æœ€å¤§æ¦‚ç‡çš„ç´¢å¼•ä½œä¸ºé¢„æµ‹ç»“æœ
    predicted_label = torch.argmax(probabilities, dim=-1).item()  # è½¬æ¢ä¸ºPythonæ•´æ•°
    # return f"æ¨¡å‹å¤„ç†åçš„æ–‡æœ¬ç»“æœ: {encoded_inputs['input_ids']}"  # ç¤ºä¾‹é€»è¾‘
    return encoded_inputs,predicted_label,probabilities

def process_csv(uploaded_file, model, tokenizer):
    # å¤„ç†CSVæ–‡ä»¶çš„é€»è¾‘
    df = pd.DataFrame(pd.read_excel(uploaded_file,index_col=False))
    df['preLabel']=0
    df['result']=''
    for i in range(10):
        input_text=df.loc[i, 'new_content']
        with torch.no_grad():
            inputs = tokenizer(input_text, truncation=True, padding='max_length', max_length=512, return_tensors="pt")
            #inputs = {k: v.cuda() for k, v in inputs.items()}
            logits = model(**inputs).logits
            pred = torch.argmax(logits, dim=-1)
            df.loc[i, 'preLabel']=pred.item()
            dict_data=load_mapping()
            df.loc[i, 'result']=dict_data.get(pred.item())
    return df  # ç¤ºä¾‹è¿”å›DataFrameå¤´éƒ¨
def create_pie_chart(data, labels):
    """æ ¹æ®ç»™å®šçš„æ•°æ®å’Œæ ‡ç­¾åˆ›å»ºé¥¼å›¾"""
    fig, ax = plt.subplots()
    ax.pie(data, labels=labels, autopct='%1.1f%%', startangle=90)
    ax.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
    return fig

 # åˆå§‹åŒ–session_state
if 'predicted_label' not in st.session_state:
    st.session_state['predicted_label']=""
if 'input_ids' not in st.session_state:
    st.session_state['input_ids']={}
if 'processed_text' not in st.session_state:
    st.session_state['processed_text'] = "" 
if 'probabilities' not in st.session_state:
    st.session_state['probabilities']={}

def main():    
    st.markdown(
        "<h1 style='text-align: center;'>ğŸ¤— æ¨¡å‹æŒ‡æ´¾è¿‡ç¨‹æ¼”ç¤º</h1>", 
        unsafe_allow_html=True
    )
    loading_palceholder=st.empty()
    with loading_palceholder.container():
        with st.spinner('æ¨¡å‹åŠ è½½ä¸­....'):
            model, tokenizer = load_model_and_tokenizer()
            dict_data=load_mapping()
    loading_palceholder.empty()
    st.success("æ¨¡å‹åŠTokenizerå·²æˆåŠŸåŠ è½½ï¼ï¼ï¼ï¼ï¼")
    
    input_type = st.radio("é€‰æ‹©è¾“å…¥ç±»å‹", ("å·¥å•å†…å®¹", "å·¥å•å†…å®¹æ–‡ä»¶(.xlsx)"))
    if input_type == "å·¥å•å†…å®¹":
        input_text = st.text_area("è¯·è¾“å…¥æ–‡æœ¬")
        if st.button("å¤„ç†å·¥å•å†…å®¹"):
            encoded_inputs,predicted_label,_= process_text(input_text, model, tokenizer)
            st.session_state.processed_text = encoded_inputs
            st.session_state['input_ids']=encoded_inputs['input_ids'].tolist()
        st.code(f'ç¼–ç åçš„å·¥å•å†…å®¹ï¼š{st.session_state.input_ids}')
        if st.button("è¾“å…¥æ¨¡å‹"):
            _,predicted_label,probabilities= process_text(input_text, model, tokenizer)
            st.session_state.predicted_label=predicted_label
            st.session_state.probabilities=probabilities.tolist()
        st.code(f'å„ä¸ªæŒ‡æ´¾éƒ¨é—¨æ¦‚ç‡ï¼š{st.session_state.probabilities}')
        st.code(f'æŒ‡æ´¾éƒ¨é—¨ä»£ç ï¼š{st.session_state.predicted_label}')
        if st.button("æŒ‡æ´¾éƒ¨é—¨æ˜ å°„"):
            st.code(f'æŒ‡æ´¾éƒ¨é—¨ï¼š{dict_data[st.session_state.predicted_label]}')
    else:
        uploaded_file = st.file_uploader("ä¸Šä¼ xlsxæ–‡ä»¶", type=["xlsx"])
        if uploaded_file is not None:
            df = pd.DataFrame(pd.read_excel(uploaded_file,index_col=False))
            st.dataframe(df)
            if st.button("è¾“å…¥æ¨¡å‹"):
                result = process_csv(uploaded_file, model, tokenizer)
                with st.expander('æŒ‡æ´¾ç»“æœå±•ç¤ºï¼š', expanded=True):
                    st.dataframe(result)
                with st.expander('æŒ‡æ´¾ç»“æœé¥¼å›¾ï¼š', expanded=True):
                    correct = result['label'] == result['preLabel']
                    correct_count = correct.sum()
                    total_count = len(result)
                    accuracy = correct_count / total_count
                    error_rate = (total_count - correct_count)/ total_count
                    data=[accuracy,error_rate]
                    labels = ['true', 'false']
                    # åˆ›å»ºé¥¼å›¾
                    fig = create_pie_chart(data, labels)
                    # å±•ç¤ºé¥¼å›¾
                    st.pyplot(fig)
        
if __name__ == "__main__":
    main()
